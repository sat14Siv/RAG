{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Index - Vector Store\n",
    "***\n",
    "* Load Documents\n",
    "* Split the text\n",
    "* Embed the texts\n",
    "* Store them in a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "DOCS_DIRECTORY = 'docs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for fp in os.listdir(DOCS_DIRECTORY):\n",
    "    loader = PyPDFLoader(file_path=DOCS_DIRECTORY+fp) # Each page is loaded as a different doc\n",
    "\n",
    "    docs_lazy = loader.lazy_load()\n",
    "\n",
    "    for doc in docs_lazy:\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunk_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = OpenAIEmbeddings()\n",
    "# embed_docs = embedding_model.embed_documents(chunk_docs)\n",
    "\n",
    "# print(f\"Embedded {len(embed_docs)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "* Add documents to the store\n",
    "* Embed them with the provided embedding model\n",
    "* Persist the vector store\n",
    "* The vector store will also retrieve the most similar documents given a query. It will embed it with the same embedding model provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Store - Embed the document chunks and add them to the vector store\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "vector_store = Chroma.from_documents(collection_name=\"doc-collection\",\n",
    "                                     documents=chunk_docs,\n",
    "                                     embedding=embedding_model,\n",
    "                                     persist_directory='./chromadb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead' metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'docs/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "page_content='described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].' metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'docs/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_docs = vector_store.similarity_search(query='What is the attention mechanism?',\n",
    "                                            k=2)\n",
    "\n",
    "for document in similar_docs:\n",
    "    print(document)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Q&A RAG\n",
    "***\n",
    "* Create a Chain, which reads the user input -> Retrieves the relevant docs from the vector store -> Adds them to the LLM context and generates a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_2_str(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def message_to_text(message):\n",
    "    return message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from the vector store, so that it can be included in the chain as it will get access to the invoke method\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template using retrieved context and user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"\\nYou are a friendly assistant in answering the user's queries. For answering the queries, you have access to the following context:\\n{context}\\n\\nINSTRUCTIONS:\\n1. Use only the provided context to answer the query.\\n2. If you do not find the necessary information to answer the question in the provided context, then respond with the following: <I do not have the necessary context to answer this query>.\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='Answer this: {query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formulate the input to the LLM. Equip it with the context from the retriever, along with the user query\n",
    "system_message = \"\"\"\n",
    "You are a friendly assistant in answering the user's queries. For answering the queries, you have access to the following context:\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Use only the provided context to answer the query.\n",
    "2. If you do not find the necessary information to answer the question in the provided context, then respond with the following: <I do not have the necessary context to answer this query>.\n",
    "\"\"\"\n",
    "\n",
    "human_message = \"Answer this: {query}\"\n",
    "\n",
    "template = ChatPromptTemplate([('system', system_message),\n",
    "                               ('human', human_message)])\n",
    "template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM to consume the context and respond to the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "            model=CHAT_MODEL,\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable pass through would just pass what we give in the input invoke\n",
    "simple_rag_chain = ({\"context\" : retriever | docs_2_str, \"query\": RunnablePassthrough()}\n",
    "                    | template\n",
    "                    | llm\n",
    "                    | message_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rheumatoid arthritis is known to disproportionately affect women, similar to conditions such as psoriatic arthritis (PsA), lupus, and fibromyalgia.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag_chain.invoke(\"Who is rheumatoid arthiritis more prevalent in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<I do not have the necessary context to answer this query>'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag_chain.invoke(\"What would be a good destination to travel to this summer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rheumatoid arthritis is known to disproportionately affect women, similar to conditions such as psoriatic arthritis (PsA), lupus, and fibromyalgia.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag_chain.invoke(\"Who is rheumatoid arthiritis more prevalent in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<I do not have the necessary context to answer this query>'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag_chain.invoke(\"Okay, well how about men?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational RAG\n",
    "* Ask follow up questions to the application\n",
    "* Enable conversation style interactions\n",
    "* Introduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rheumatoid arthritis is known to disproportionately affect women, similar to conditions such as psoriatic arthritis (PsA), lupus, and fibromyalgia.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag_chain.invoke(\"Who is rheumatoid arthiritis more prevalent in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<I do not have the necessary context to answer this query>'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag_chain.invoke(\"How about men?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_query_prompt = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history,\n",
    "formulate a standalone question which can be understood\n",
    "without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is\n",
    "\"\"\"\n",
    "\n",
    "contextualized_query_template = ChatPromptTemplate.from_messages([('system', contextualized_query_prompt),\n",
    "                                                         MessagesPlaceholder('chat_history'),\n",
    "                                                         ('human', \"{input}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_query_chain = contextualized_query_template| llm | message_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When do we expect it to be surpassed?'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualized_query_chain.invoke({\"input\": \"So, by when we expect it to be overtaken?\", \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in October 2023, China is the most populous country in the world, followed closely by India. However, demographic trends indicate that India may surpass China in population soon, if it hasn't already. For the most current population figures, it's best to consult the latest data from reliable sources such as the United Nations or the World Bank.\n",
      "When is India expected to overtake China in population?\n"
     ]
    }
   ],
   "source": [
    "llm_chain = llm | message_to_text\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "query1 = \"What is the most populous country in the world?\"\n",
    "answer1 = llm_chain.invoke(query1)\n",
    "print(answer1)\n",
    "\n",
    "\n",
    "chat_history.extend([HumanMessage(query1),\n",
    "                     AIMessage(answer1)])\n",
    "\n",
    "query2 = \"So, by when we expect it to be overtaken?\"\n",
    "rephrased_query2 = contextualized_query_chain.invoke({\"input\": query2, \"chat_history\": chat_history})\n",
    "print(rephrased_query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History Aware Retriever\n",
    "* We will use a prebuilt chain provided by langchain, but the concept is the same as we saw earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This retriever adds information to the query from the chat history and then uses that for retrieval\n",
    "history_aware_retriever = create_history_aware_retriever(llm=llm, \n",
    "                                                         retriever=retriever,\n",
    "                                                         prompt=contextualized_query_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001A8DBFF5910>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A8C9E35BC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\nGiven a chat history and the latest user question\\nwhich might reference context in the chat history,\\nformulate a standalone question which can be understood\\nwithout the chat history. Do NOT answer the question,\\njust reformulate it if needed and otherwise return it as is\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A8E0DC00D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A8DD5FAA10>, root_client=<openai.OpenAI object at 0x000001A8D9BEA550>, root_async_client=<openai.AsyncOpenAI object at 0x000001A8DD728AD0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_retries=2)\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001A8DBFF5910>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='483990e5-e913-4397-8c65-ede4b3b31787', metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'docs/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}, page_content='To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].'),\n",
       " Document(id='7b6879b0-f01c-4b6c-b57e-281e63a917d4', metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'docs/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU'),\n",
       " Document(id='4fede6c6-453a-4c7b-b784-8099667bf538', metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 8, 'page_label': '9', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'docs/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}, page_content='For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We'),\n",
       " Document(id='86bda808-279b-4b2d-a005-2bf70a05c865', metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'docs/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}, page_content='Google Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever.invoke({\"input\":\"Transformers\", \"chat_history\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff Documents Chain\n",
    "* Takes a list of documents as input and feeds them to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversational_rag = ({\"context\" : history_aware_retriever | docs_2_str, \"query\": RunnablePassthrough.assign(query=lambda x: x['input'])}\n",
    "#                     | template\n",
    "#                     | llm\n",
    "#                     | message_to_text\n",
    "#                     )\n",
    "\n",
    "qa_system_prompt = \"\"\"\n",
    "You have access to the following context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Answer the user's query using the provided context\n",
    "2. Do not make up any responses, only provide an answer if you can find related content in the provided context\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([('system', qa_system_prompt),\n",
    "                                              ('human', \"{input}\")])\n",
    "\n",
    "# This is a chain which consumes a list of documents and passes it to an llm\n",
    "answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever=history_aware_retriever,\n",
    "                                combine_docs_chain=answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Who is prone to rheumatoid arthritis?', 'chat_history': [], 'context': [Document(id='086737cd-9154-4727-b037-d0c7755d544b', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 0, 'page_label': '196', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='reactive arthritis, and arthritis associated \\nwith inflammatory bowel disease. Of these, \\nPsA, with its dermatological manifesta-\\ntions, particularly psoriasis, represents a \\nsignificant challenge for women. Psoriasis, \\na skin disease characterized by skin mani -\\nfestations ranging from mild, localized \\nplaques to severe, generalized forms, has a \\ndifferent clinical course and response to \\ntreatment in women and in men (2). This \\ndifference may be due to several factors,'), Document(id='d1a0bd84-c52e-4a11-9fd1-d7ff30bb7c0f', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 1, 'page_label': '197', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='incidence, and gender-specific challenges \\nof these conditions (9). PsA, as happens in \\nconditions such as rheumatoid arthritis, lu-\\npus, and fibromyalgia, is known to dispro-\\nportionately affect women. Recent data \\nsuggest that in the United States alone, ap-\\nproximately 53.2 million adults (21.2%) \\nhave been diagnosed with arthritis, and \\nthere is a significant gender gap, with a \\nhigher prevalence of physician-diagnosed \\narthritis in women than in men (10). On the'), Document(id='09482425-5316-4de2-9a60-1954f3a4ed0c', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 1, 'page_label': '197', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='higher prevalence of physician-diagnosed \\narthritis in women than in men (10). On the \\nother hand, psoriasis, which is often associ -\\nated with PsA, affects more than 8 million \\nAmericans and 125 million people world-\\nwide, or 2-3% of the world’s population, \\nand it is significant that approximately 30% \\nof people with psoriasis also develop PsA \\n(11). The manifestation of psoriasis in \\nwomen, particularly those with PsA, is not \\njust a skin condition but a global health'), Document(id='22697cf0-1a0b-4165-b7bf-6d93d01cef8e', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 7, 'page_label': '203', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='nosed arthritis - United States, 2019-2021. \\nMMWR Morb Mortal Wkly Rep 2023; 72: \\n1101-7.\\n11.  Bu J, Ding R, Zhou L, Chen X, Shen E. Epide -\\nmiology of psoriasis and comorbid diseases: a \\nnarrative review. Front Immunol 2022; 13: \\n880201. \\n12.  Chimenti MS, Perricone C, D’Antonio A, Fer -\\nraioli M, Conigliaro P, Triggianese P, et al. Ge -\\nnetics, epigenetics, and gender impact in axial-\\nspondyloarthritis susceptibility: an update on \\ngenetic polymorphisms and their sex related')], 'answer': 'The provided context does not specifically mention who is prone to rheumatoid arthritis. However, it does indicate that conditions like psoriatic arthritis (PsA), rheumatoid arthritis, lupus, and fibromyalgia disproportionately affect women. Therefore, it can be inferred that women may be more prone to rheumatoid arthritis, similar to these other conditions.'}\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query1 = \"Who is prone to rheumatoid arthritis?\"\n",
    "answer1 = rag_chain.invoke({\"input\": query1, \"chat_history\":chat_history})\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is prone to rheumatoid arthritis?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(id='086737cd-9154-4727-b037-d0c7755d544b', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 0, 'page_label': '196', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='reactive arthritis, and arthritis associated \\nwith inflammatory bowel disease. Of these, \\nPsA, with its dermatological manifesta-\\ntions, particularly psoriasis, represents a \\nsignificant challenge for women. Psoriasis, \\na skin disease characterized by skin mani -\\nfestations ranging from mild, localized \\nplaques to severe, generalized forms, has a \\ndifferent clinical course and response to \\ntreatment in women and in men (2). This \\ndifference may be due to several factors,'),\n",
       "  Document(id='d1a0bd84-c52e-4a11-9fd1-d7ff30bb7c0f', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 1, 'page_label': '197', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='incidence, and gender-specific challenges \\nof these conditions (9). PsA, as happens in \\nconditions such as rheumatoid arthritis, lu-\\npus, and fibromyalgia, is known to dispro-\\nportionately affect women. Recent data \\nsuggest that in the United States alone, ap-\\nproximately 53.2 million adults (21.2%) \\nhave been diagnosed with arthritis, and \\nthere is a significant gender gap, with a \\nhigher prevalence of physician-diagnosed \\narthritis in women than in men (10). On the'),\n",
       "  Document(id='09482425-5316-4de2-9a60-1954f3a4ed0c', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 1, 'page_label': '197', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='higher prevalence of physician-diagnosed \\narthritis in women than in men (10). On the \\nother hand, psoriasis, which is often associ -\\nated with PsA, affects more than 8 million \\nAmericans and 125 million people world-\\nwide, or 2-3% of the world’s population, \\nand it is significant that approximately 30% \\nof people with psoriasis also develop PsA \\n(11). The manifestation of psoriasis in \\nwomen, particularly those with PsA, is not \\njust a skin condition but a global health'),\n",
       "  Document(id='22697cf0-1a0b-4165-b7bf-6d93d01cef8e', metadata={'creationdate': '2024-09-03T10:49:12+02:00', 'creator': 'Adobe InDesign 19.5 (Macintosh)', 'moddate': '2024-09-03T10:49:12+02:00', 'page': 7, 'page_label': '203', 'producer': 'Adobe PDF Library 17.0', 'source': 'docs/rheumatoid_arthritis_in_women.pdf', 'total_pages': 9, 'trapped': '/False'}, page_content='nosed arthritis - United States, 2019-2021. \\nMMWR Morb Mortal Wkly Rep 2023; 72: \\n1101-7.\\n11.  Bu J, Ding R, Zhou L, Chen X, Shen E. Epide -\\nmiology of psoriasis and comorbid diseases: a \\nnarrative review. Front Immunol 2022; 13: \\n880201. \\n12.  Chimenti MS, Perricone C, D’Antonio A, Fer -\\nraioli M, Conigliaro P, Triggianese P, et al. Ge -\\nnetics, epigenetics, and gender impact in axial-\\nspondyloarthritis susceptibility: an update on \\ngenetic polymorphisms and their sex related')],\n",
       " 'answer': 'The provided context does not specifically mention who is prone to rheumatoid arthritis. However, it does indicate that conditions like psoriatic arthritis (PsA), rheumatoid arthritis, lupus, and fibromyalgia disproportionately affect women. Therefore, it can be inferred that women may be more prone to rheumatoid arthritis, similar to these other conditions.'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context primarily discusses the prevalence and challenges of psoriatic arthritis (PsA) and psoriasis in women, noting that these conditions disproportionately affect women compared to men. However, it does not provide specific information about men or their experiences with these conditions. Therefore, I cannot provide a detailed answer regarding men based on the given context.\n"
     ]
    }
   ],
   "source": [
    "chat_history.extend([HumanMessage(query1),\n",
    "                     AIMessage(answer1['answer'])])\n",
    "\n",
    "query2 = \"How about men?\"\n",
    "answer2 = rag_chain.invoke({\"input\": query2, \"chat_history\":chat_history})\n",
    "print(answer2['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi User Conversational RAG (Multi-User Chatbot)\n",
    "***\n",
    "* Store each user's chat history in a db\n",
    "* Retrieve the existing history if a new chat with an existing user is started\n",
    "* For each new user, create a unique identifier (uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"rag_app.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "def connect_to_db(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    conn.row_factory = sqlite3.Row # Return rows as dictionaries\n",
    "    return conn\n",
    "\n",
    "# Create table to store logs\n",
    "def create_application_logs(db_name):\n",
    "    conn = connect_to_db(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Use conn to interact with the db\n",
    "    cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS application_logs\n",
    "                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                 session_id TEXT,\n",
    "                 user_query TEXT,\n",
    "                 model_response TEXT,\n",
    "                 model TEXT,\n",
    "                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)\"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return None\n",
    "\n",
    "# Insert\n",
    "def insert_application_logs(db_name, session_id, user_query, model_response, model):\n",
    "    conn = connect_to_db(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"INSERT INTO application_logs (session_id, user_query, model_response, model) VALUES\n",
    "                 (?, ?, ?, ?)\"\"\", (session_id, user_query, model_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "# Retrieve\n",
    "def get_chat_history(db_name, session_id):\n",
    "    conn = connect_to_db(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    result = cursor.execute(\"\"\"SELECT user_query, model_response\n",
    "                                from application_logs\n",
    "                                where session_id= ?\n",
    "                                ORDER BY created_at\"\"\", (session_id,))\n",
    "\n",
    "    chat_history = []\n",
    "    for row in result.fetchall():\n",
    "        query = row['user_query']\n",
    "        response = row['model_response']\n",
    "        chat_history.extend([HumanMessage(query),\n",
    "                            AIMessage(response)])\n",
    "        \n",
    "    conn.close()\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_application_logs(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What does the Attention is all you need paper talk about?\n",
      "Response:  The \"Attention Is All You Need\" paper introduces the Transformer model architecture, which utilizes self-attention mechanisms to process sequences without relying on recurrence. It highlights the effectiveness of attention mechanisms in various tasks such as reading comprehension, abstractive summarization, and textual entailment. The paper emphasizes that attention allows for modeling dependencies in sequences regardless of their distance, and it presents a novel approach that moves away from traditional recurrent networks.\n"
     ]
    }
   ],
   "source": [
    "# Based on session, retrieve chat history\n",
    "session_id = str(uuid.uuid4())\n",
    "query = \"What does the Attention is all you need paper talk about?\"\n",
    "chat_history = get_chat_history(DB_NAME, session_id)\n",
    "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "\n",
    "insert_application_logs(DB_NAME, session_id, query, response['answer'], CHAT_MODEL)\n",
    "print(\"Query: \", query)\n",
    "print(\"Response: \", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Who introduced this idea?\n",
      "Response:  The idea of the Transformer models was introduced by Ashish Vaswani and Illia Polosukhin, who designed and implemented the first Transformer models. Noam Shazeer also played a significant role by proposing scaled dot-product attention, multi-head attention, and the parameter-free position representation.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who introduced this idea?\"\n",
    "chat_history = get_chat_history(DB_NAME, session_id)\n",
    "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "insert_application_logs(DB_NAME, session_id, query, response['answer'], CHAT_MODEL) \n",
    "\n",
    "print(\"Query: \", query)\n",
    "print(\"Response: \", response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
